## 时代赋予数据湖新的意义与能力

前面我们说过

#### 数据集成能力

数据湖需要具备完善的数据获取和数据发布能力。数据湖需要能支撑各种各样的数据源，并能从相关的数据源中获取全量/增量数据；然后规范存储。数据湖能将数据分析处理的结果推送到合适的存储引擎中，满足不同的应用访问需求。

接入不同数据源，包括数据库中的表（关系型或者非关系型）、各种格式的文件（csv、json、文档等）、数据流、ETL工具（Kafka、Logstash、DataX等）转换后的数据、应用API获取的数据（如日志等）；

自动生成元数据信息，确保进入数据湖的数据都有元数据，提供统一的接入方式，如统一的API或者接口；

#### 数据存储能力

数据湖存储的数据量巨大且来源多样，数据湖应该支持异构和多样的存储，如HDFS、HBase、Hive等，所以数据湖需要提供足够用的数据存储能力，这个存储保存一个企业/组织中的所有数据。

数据湖可以存储海量的任意类型的数据，包括结构化、半结构化和非结构化数据，这就要求数据湖对于大数据的支持，包括超大规模存储以及可扩展的大规模数据处理能力。

#### 数据多样化处理能力

数据湖需要具备多样化的分析能力，包括但不限于批处理、流式计算、交互式分析以及机器学习；同时，还需要提供一定的任务调度和管理能力。

数据湖中拥有海量的数据，对于用户来说，明确知道数据湖中数据的位置，快速的查找到数据，是一个非常重要的功能。

#### 数据治理能力

数据湖需要具备完善的数据管理能力（完善的元数据），可以管理各类数据相关的要素，包括数据源、数据格式、连接信息、数据schema、权限管理等。

数据湖需要具备完善的数据生命周期管理能力。不光需要存储原始数据，还需要能够保存各类分析处理的中间结果，并完整的记录数据的分析处理过程，能帮助用户完整详细追溯任意一条数据的产生过程，跟踪数据时间旅行，提供不同版本的数据，便于进行数据回溯和分析

数据湖需要具备自动动提取元数据信息，并统一存储，对元数据进标签和分类，建立统一的数据目录，建立数据血缘，梳理上下游的脉络关系，有助于数据问题定位分析、数据变更影响范围评估、数据价值评估

### 总结

数据集成真正从大数据的角度来看，才能明白其中的挑战。一个运行了20多年的数据架构，必然有其合理性。也正是因为年代久远，存量过多，才导致举步维艰。在Cloud和5G时代，超密度网络集成和大数据洞察需求给电信供应商带来新的挑战，从数据仓库到数据湖，不仅仅架构的变革，更是思维方式的升级。本文尝试梳理数据架构的演进过程。

数据湖的主要思想是对企业中的所有数据进行统一存储，从原始数据（源系统数据的精确副本）转换为用于报告、可视化、分析和机器学习等各种任务的目标数据。

数据湖从本质上来讲，是一种企业**数据架构方法**，物理实现上则是一个数据平台，用来集中化存储企业内海量的、多来源，多种类的数据，并支持对数据进行快速加工和分析。从实现方式来看，目前Hadoop是最常用的部署数据湖的技术，但并不意味着数据湖就是指Hadoop集群。为了应对不同业务需求的特点，MPP数据库+Hadoop集群+传统数据仓库这种“混搭”架构的数据湖也越来越多出现在企业信息化建设规划中。

早期**数据湖的就是原始数据保存区**，虽然这个概念国内谈的少，但**绝大部分互联网公司都已经有了**。国内一般把整个HDFS叫做数据仓库（广义），即存放所有数据的地方，而国外一般叫数据湖（data lake）

**许多人认为“数据湖”正在迅速发展成为下一代数据仓库**。对于那些不熟悉这个概念的人来说，数据湖是多结构数据的系统或存储库，它们以原始格式和模式存储，通常作为对象“blob”或文件存储。

搭建数据湖容易，但是让数据湖发挥价值是很难。最终数据湖只是一直往里面灌数据，而应用场景极少，没有输出或者极少输出，形成**单向湖**。企业的业务是实时在变化的，这代表着沉积在数据湖中的数据定义、数据格式实时都在发生的转变，企业的大型数据湖对企业数据治理（Data Governance）提升了更高的要求。大部分使用数据湖的企业在数据真的需要使用的时候，往往因为数据湖中的数据质量太差而无法最终使用。数据湖，被企业当成一个大数据的垃圾桶，最终数据湖成为臭气熏天，存储在Hadoop当中的数据成为无人可以清理的数据沼泽.

 

3）“可管理”：数据湖应该提供完善的数据管理能力。既然数据要求“保真性”和“灵活性”，那么至少数据湖中会存在两类数据：原始数据和处理后的数据。数据湖中的数据会不断的积累、演化。因此，对于数据管理能力也会要求很高，至少应该包含以下数据管理能力：数据源、数据连接、数据格式、数据schema（库/表/列/行）。同时，数据湖是单个企业/组织中统一的数据存放场所，因此，还需要具有一定的权限管理能力。

4）“可追溯”：数据湖是一个组织/企业中全量数据的存储场所，需要对数据的全生命周期进行管理，包括数据的定义、接入、存储、处理、分析、应用的全过程。一个强大的数据湖实现，需要能做到对其间的任意一条数据的接入、存储、处理、消费过程是可追溯的，能够清楚的重现数据完整的产生过程和流动过程。

在计算方面，个人认为数据湖对于计算能力要求其实非常广泛，完全取决于业务对于计算的要求。

5）丰富的计算引擎。从批处理、流式计算、交互式分析到机器学习，各类计算引擎都属于数据湖应该囊括的范畴。一般情况下，数据的加载、转换、处理会使用批处理计算引擎；需要实时计算的部分，会使用流式计算引擎；对于一些探索式的分析场景，可能又需要引入交互式分析引擎。随着大数据技术与人工智能技术的结合越来越紧密，各类机器学习/深度学习算法也被不断引入，例如TensorFlow/PyTorch框架已经支持从HDFS/S3/OSS上读取样本数据进行训练。因此，对于一个合格的数据湖项目而言，计算引擎的可扩展/可插拔，应该是一类基础能力。

6）多模态的存储引擎。理论上，数据湖本身应该内置多模态的存储引擎，以满足不同的应用对于数据访问需求（综合考虑响应时间/并发/访问频次/成本等因素）。但是，在实际的使用过程中，数据湖中的数据通常并不会被高频次的访问，而且相关的应用也多在进行探索式的数据应用，为了达到可接受的性价比，数据湖建设通常会选择相对便宜的存储引擎（如S3/OSS/HDFS/OBS），并且在需要时与外置存储引擎协同工作，满足多样化的应用需求。



### 关于数据湖的错误认知

错误认知1：数据湖与数据仓库，必须二选一

> 人们普遍建议在数据湖和数据仓库之间二选一，但这是错误的。

错误认知2：数据仓库就是一个数据湖

> 这种想法会诱使你放弃数据湖，将所有数据都扔进数据仓库中。

错误认知3：数据湖只能用Hadoop来实现

> 你会经常发现有讨论和示例将数据湖等同于Hadoop或者Hadoop相关供应商技术栈，这会给人一种错觉：数据湖和Hadoop特定的技术紧密相关。

错误认知4：数据湖仅用于“存储”数据

> 在这种情况下，数据湖只是一个存储你所有数据的地方。你只需要所有数据放入数据湖，而后启用新的数据管理模型就可以大功造成，这就和将所有的文件都放进笔记本电脑上超大硬盘中的“无标题文件夹”一样。

错误认知5：数据湖仅存储“原始”数据

> 和错误认知2相关，“把所有数据都倒进数仓”的方法表示，数据湖不会增加价值，原因是只有原始数据驻留在数据湖中。他们主张：“如果数据湖只处理原始数据，那么就不用担心数据湖了，只需将所有的原始数据或者已被处理的数据转存至数仓中”。

错误认知6：数据湖仅适用于“大”数据

> 如果你花时间阅读过数据湖的相关资料，你会认为数据湖只有一种类型，看起来像里海（它是一个湖，尽管名字中有“海”）。人们将数据湖描述成一个庞大的、包容一切的实体，旨在保存所有的知识，因此只会有一个企业大数据湖或者大数据架构的同义词。

错误认知7：数据湖没有安全保障

错误认知8：数据湖会变成数据沼泽

> 曾有一篇文章评论数据湖最终会变成数据沼泽，因为它们只是存储，缺乏治理、管理，没有数据生命周期/保留策略，也没有元数据。

### 参考资料

[[DataLake](https://martinfowler.com/bliki/DataLake.html)](https://martinfowler.com/bliki/DataLake.html)
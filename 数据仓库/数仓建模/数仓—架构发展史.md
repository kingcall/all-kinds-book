[toc]
## 发展过程
![image-20201205185645146](https://kingcall.oss-cn-hangzhou.aliyuncs.com/blog/img/2020/12/05/18:57:09-18:56:45-image-20201205185645146.png)
- 数据仓库是一个面向主题的（Subject Oriented）、集成的（Integrate）、相对稳定的（Non-Volatile）、反映历史变化（Time Variant）的数据集合，用于支持管理决策

![image-20201205185732583](https://kingcall.oss-cn-hangzhou.aliyuncs.com/blog/img/2020/12/05/18:57:33-image-20201205185732583.png)



- 数据仓库有两个环节：数据仓库的构建与数据仓库的应用。
- 数据仓库是伴随着企业信息化发展起来的，在企业信息化的过程中，随着信息化工具的升级和新工具的应用，**数据量变的越来越大，数据格式越来越多，决策要求越来越苛刻，数据仓库技术也在不停的发展**

![image-20201205185750029](https://kingcall.oss-cn-hangzhou.aliyuncs.com/blog/img/2020/12/05/18:57:50-image-20201205185750029.png)

### 经典数仓

在离线数仓之前(基于大数据架构之前)，有很多传统的数仓技术，例如基于Teradata的数据仓库

### 离线数仓(离线大数据架构)

随着互联网时代来临，数据量暴增，开始使用大数据工具来替代经典数仓中的传统工具。此时仅仅是工具的取代，架构上并没有根本的区别，可以把这个架构叫做离线大数据架构。

![image-20201205185819794](https://kingcall.oss-cn-hangzhou.aliyuncs.com/blog/img/2020/12/05/18:58:20-image-20201205185819794.png)

#### 数仓分层

在这个过程中，慢慢演化出了数仓的分层设计

**ODS**，操作数据层，保存原始数据；

**DWD**，数据仓库明细层，根据主题定义好事实与维度表，保存最细粒度的事实数据；

**DM**，数据集市/轻度汇总层，在DWD层的基础之上根据不同的业务需求做轻度汇总；

#### 工具

- 典型的数仓存储是HDFS/Hive，ETL可以是MapReduce脚本或HiveSQL(spark)

### Lambda架构
![image-20201205193359971](https://kingcall.oss-cn-hangzhou.aliyuncs.com/blog/img/2020/12/05/19:34:01-image-20201205193359971.png)

后来随着业务实时性要求的不断提高，人们开始在离线大数据架构基础上加了一个加速层，使用流处理技术直接完成那些实时性要求较高的指标计算，这便是Lambda架构。

为了计算一些实时指标，就在原来离线数仓的基础上增加了一个实时计算的链路，并对数据源做流式改造（即把数据发送到消息队列），实时计算去订阅消息队列，直接完成指标增量的计算，推送到下游的数据服务中去，**由数据服务层完成离线&实时结果的合并**。

> 流处理计算的指标批处理依然计算，最终以批处理为准，即每次批处理计算后会覆盖流处理的结果。（这仅仅是流处理引擎不完善做的折中）



![image-20201205190005883](https://kingcall.oss-cn-hangzhou.aliyuncs.com/blog/img/2020/12/05/19:34:33-19:00:06-image-20201205190005883.png)



![image-20201205190029719](https://kingcall.oss-cn-hangzhou.aliyuncs.com/blog/img/2020/12/05/19:00:30-image-20201205190029719.png)

#### 存在的问题
- 同样的需求需要**开发两套一样的代码**

> 这是Lambda架构最大的问题，两套代码不仅仅意味着开发困难（同样的需求，一个在批处理引擎上实现，一个在流处理引擎上实现，还要分别构造数据测试保证两者结果一致），后期维护更加困难，比如需求变更后需要分别更改两套代码，独立测试结果，且两个作业需要同步上线。

- **资源占用增多**：同样的逻辑计算两次，整体资源占用会增多（多出实时计算这部分）·
- 实时链路和离线链路计算结果容易让人误解，昨天看到的数据和今天看到的**数据不一致** 
- **下游处理复杂，**需要整合实时和离线处理结果****

### Kappa架构
![image-20201205190120511](https://kingcall.oss-cn-hangzhou.aliyuncs.com/blog/img/2020/12/05/19:01:21-image-20201205190120511.png)
- 再后来，实时的业务越来越多，事件化的数据源也越来越多，实时处理从次要部分变成了主要部分，架构也做了相应调整，出现了以实时事件处理为核心的Kappa架构。
- Lambda架构虽然满足了实时的需求，但带来了更多的开发与运维工作，**其架构背景是流处理引擎还不完善，流处理的结果只作为临时的、近似的值提供参考**。后来随着Flink等流处理引擎的出现，流处理技术很成熟了，这时为了解决两套代码的问题，LickedIn 的Jay Kreps提出了Kappa架构
- Kappa架构可以认为是Lambda架构的简化版（只要移除lambda架构中的批处理部分即可）。在Kappa架构中，需求修改或历史数据重新处理都通过上游重放完成。

#### Kappa架构的重新处理过程
![image-20201205190144019](https://kingcall.oss-cn-hangzhou.aliyuncs.com/blog/img/2020/12/05/19:01:44-image-20201205190144019.png)
- 选择一个具有重放功能的、能够保存历史数据并支持多消费者的消息队列，根据需求设置历史数据保存的时长，比如Kafka，可以保存全部历史数据。
- 当某个或某些指标有重新处理的需求时，按照新逻辑写一个新作业，然后从上游消息队列的最开始重新消费，把结果写到一个新的下游表中。
- .当新作业赶上进度后，应用切换数据源，使用新产生的新结果表。停止老的作业，删除老的结果表。

#### 存在的问题
- Kappa架构最大的问题是流式重新处理历史的吞吐能力会低于批处理，但这个可以通过增加计算资源来弥补

### 混合架构
![image-20201205190216294](https://kingcall.oss-cn-hangzhou.aliyuncs.com/blog/img/2020/12/05/19:02:16-image-20201205190216294.png)
- 前面介绍了Lambda架构与Kappa架构的含义及优缺点，在真实的场景中，很多时候并不是完全规范的Lambda架构或Kappa架构，可以是两者的混合，比如大部分实时指标使用Kappa架构完成计算，少量关键指标（比如金额相关）**使用Lambda架构用批处理重新计算，增加一次校对过程**。（1）
- Kappa架构并不是中间结果完全不落地，现在很多大数据系统都需要支持机器学习（离线训练），所以实时中间结果需要落地对应的存储引擎供机器学习使用，另外有时候还需要对明细数据查询，这种场景也需要把实时明细层写出到对应的引擎中。（2）
- 随着数据多样性的发展，**数据仓库这种提前规定schema的模式显得越来难以支持灵活的探索&分析需求，这时候便出现了一种数据湖技术，即把原始数据全部缓存到某个大数据存储上，后续分析时再根据需求去解析原始数据**。简单的说**，数据仓库模式是schema  on  write，数据湖模式是schema on read**

### 准实时数仓
### 实时数仓
- 实时数据仓库以满足实时化&自动化决策需求
- 主要解决三个问题 1. 数据实时性 2. 缓解集群压力  3. 缓解业务库压力。
![image-20201205190240519](https://kingcall.oss-cn-hangzhou.aliyuncs.com/blog/img/2020/12/05/19:02:41-image-20201205190240519.png)



![image-20201205190300936](https://kingcall.oss-cn-hangzhou.aliyuncs.com/blog/img/2020/12/05/19:03:01-image-20201205190300936.png)

- 第一层DWD公共实时明细层 实时计算订阅业务数据消息队列，然后通过数据清洗、多数据源join、流式数据与离线维度信息等的组合，将一些相同粒度的业务系统、维表中的维度属性全部关联到一起，增加数据易用性和复用性，得到最终的实时明细数据。这部分数据有两个分支，一部分直接落地到ADS，供实时明细查询使用，一部分再发送到消息队列中，供下层计算使用
- 第二层DWS公共实时汇总层 以数据域+业务域的理念建设公共汇总层，与离线数仓不同的是，这里汇总层分为轻度汇总层和高度汇总层，并同时产出，轻度汇总层写入ADS，用于前端产品复杂的olap查询场景，满足自助分析；高度汇总层写入Hbase，用于前端比较简单的kv查询场景，提升查询性能，比如产出报表等

#### 实时数仓的的实施关键点
1. 端到端数据延迟、数据流量量的监控
2. 故障的快速恢复能⼒力力 数据的回溯处理理，系统⽀支持消费指定时间端内的数据
3. 实时数据从实时数仓中查询，T+1数据借助离线通道修正 
4. 数据地图、数据⾎血缘关系的梳理理 
5. 业务数据质量量的实时监控，初期可以根据规则的⽅方式来识别质量量状况

#### 数据保障
- 集团每年都有双十一等大促，大促期间流量与数据量都会暴增。实时系统要保证实时性，相对离线系统对数据量要更敏感，对稳定性要求更高
- 所以为了应对这种场景，还需要在这种场景下做两种准备： 1.大促前的系统压测； 2.大促中的主备链路保障

![image-20201205190340592](https://kingcall.oss-cn-hangzhou.aliyuncs.com/blog/img/2020/12/05/19:03:41-image-20201205190340592.png)



![image-20201205190402335](https://kingcall.oss-cn-hangzhou.aliyuncs.com/blog/img/2020/12/05/19:04:02-image-20201205190402335.png)

### 数据湖
- 数据湖以支持大量&复杂数据类型（文本、图像、视频、音频）

### Pravega(流式存储)
- 想要统一流批处理的大数据处理架构，其实对存储有混合的要求
> 对于来自序列旧部分的历史数据，需要提供高吞吐的读性能，即catch-up read对于来自序列新部分的实时数据，需要提供低延迟的 append-only 尾写 tailing write 以及尾读 tailing read

![image-20201205190446684](https://kingcall.oss-cn-hangzhou.aliyuncs.com/blog/img/2020/12/05/19:04:47-image-20201205190446684.png)

存储架构最底层是基于可扩展分布式云存储，中间层表示日志数据存储为 Stream 来作为共享的存储原语，然后基于 Stream 可以向上提供不同功能的操作:如消息队列，NoSQL，流式数据的全文搜索以及结合 Flink 来做实时和批分析。换句话说，Pravega 提供的 Stream 原语可以避免现有大数据架构中原始数据在多个开源存储搜索产品中移动而产生的数据冗余现象，其在存储层就完成了统一的数据湖。

![image-20201205190514760](https://kingcall.oss-cn-hangzhou.aliyuncs.com/blog/img/2020/12/05/19:05:15-image-20201205190514760.png)

提出的大数据架构，以 Apache Flink 作为计算引擎，通过统一的模型/API来统一批处理和流处理。以 Pavega 作为存储引擎，为流式数据存储提供统一的抽象，**使得对历史和实时数据有一致的访问方式**。两者统一形成了从存储到计算的闭环，能够同时应对高吞吐的历史数据和低延时的实时数据。同时 Pravega 团队还开发了 Flink-Pravega Connector，为计算和存储的整套流水线提供 Exactly-Once 的语义。

### 总结

#### Kappa对比Lambda架构

![image-20201205190542413](https://kingcall.oss-cn-hangzhou.aliyuncs.com/blog/img/2020/12/05/19:05:42-image-20201205190542413.png)
- 在真实的场景中，很多时候**并不是完全规范的Lambda架构或Kappa架构**，可以是两者的混合，比如大部分实时指标使用Kappa架构完成计算，**少量关键指标（比如金额相关）使用Lambda架构用批处理重新计算，增加一次校对过程**。
- 这两个架构都是实时架构

#### 实时数仓与离线数仓的对比 
- 离线数据仓库主要基于sqoop、hive等技术来构建T+1的离线数据，通过定时任务每天拉取增量量数据导 ⼊入到hive表中，然后创建各个业务相关的主题维度数据，对外提供T+1的数据查询接⼝口。 实时数仓当 前主要是基于数据采集⼯工具，如canal等将原始数据写⼊入到Kafka这样的数据通道中，最后⼀一般都是写 ⼊入到类似于HBase这样存储系统中，对外提供分钟级别、甚⾄至秒级别的查询⽅方案。
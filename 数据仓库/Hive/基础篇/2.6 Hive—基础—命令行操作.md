[TOC]

## Hive 命令行

$HIVE_HOME/bin/hive 是hive 提供的一个交互式终端，其实就是一个shell 脚本，主要用来做交互式查询

但是自从Hive 0.11引入了HiveServer2之后，就引入了另外一个终端，那就是beeline,它是基于JDBC 的SQL 客户端，所以hive 命令终端慢慢的也被弃用

hive-cli是一个遗留工具，它有两个主要的使用场景。第一个是它作为Hadoop上SQL的**重客户端**，第二个是它作为hiveserver(也就是现在说的"HiveServer1")的命令行工具。但是自从hive1.0 开始hiveserver已被弃用并从代码库中删除，并被替换为HiveServer2因此第二个使用场景不再适用。对于第一个使用场景，Beeline提供或应该提供相同的功能，但实现方式与hivecli不同。

本来hive-cli应该被弃用，但是因为hive-cli的广泛使用，所以社区做了基于beeline的新的客户端，这样做的目的是使用hive-cli不需要对现有用户脚本进行任何或最小的更改。因为新的hive-cli某些特性还是使用的是旧的hive-cli，所以你可以在使用如下配置来开启

```
export USE_DEPRECATED_CLI=false
```

每次当你启动一个hive 的客户端的时候，你可以看到机器上起了一个对应的进程，你可以多起几个试试看

![image-20201223210635996](https://kingcall.oss-cn-hangzhou.aliyuncs.com/blog/img/2020/12/23/21:06:37-image-20201223210635996.png)

### -e 行里模式

在 -e 参数的加持下，我们可以不用进入hive 的命令行，而是直接使用hive 命令进行数据查询，这个时候sql 就是我们在命令行里输入的SQL，下面我们感受一下

```
hive -e 'select a.foo from pokes a'
```

你也可以在命令行里指定其他hive 的参数

```
hive -e 'select a.foo from pokes a' --hiveconf hive.exec.scratchdir=/opt/my/hive_scratch --hiveconf mapred.reduce.tasks=1
```

![image-20201223210948848](https://kingcall.oss-cn-hangzhou.aliyuncs.com/blog/img/2020/12/23/21:09:49-image-20201223210948848.png)

但是这种情况下我们用的更多是重定向，也就是将我们的执行结果重定向到文件里，尤其是输出内容比较多的时候

```
 hive -e "select * from ods.u_data_new limit 10" > out.txt
```

### -f 文件模式

Hive可以运行保存在文件里面的一条或多条的语句，只要用-f参数，一般情况下，保存这些Hive查询语句的文件通常用.q或者.hql后缀名，但是这不是必须的，你也可以保存你想要的后缀名。

```
hive -f /home/my/hive-script.sql
```

![image-20201223211248578](https://kingcall.oss-cn-hangzhou.aliyuncs.com/blog/img/2020/12/23/21:12:49-image-20201223211248578.png)

这种方式是企业比较常用的方式，就是我们使用文件的形式将我们的ETL SQL 保存下来，然后交给我们的调度系统

### source 执行外部sql 模式

这种方式可以在我们进入hive 的命令行之后执行SQL，但是说实话，这种看起来比较优美，但是却用的很少

```
hive> source /home/wyp/Documents/test.sql
```

![image-20201223212059187](https://kingcall.oss-cn-hangzhou.aliyuncs.com/blog/img/2020/12/23/21:20:59-image-20201223212059187.png)



### set 设置模式

这个命令可以让我们进入命令行之后就进行很多的设置，其实就是设置很多参数的值，从而达到调节我们sql 的运行环境

#### set 查看全部配置

在我们没有其他参数，单独使用这个命令的时候，就是列出我们的全部参数设置,这个是当前的最新参数也就是说可能是我们覆盖默认配置之后的值

![image-20201223214631284](https://kingcall.oss-cn-hangzhou.aliyuncs.com/blog/img/2020/12/23/21:46:31-image-20201223214631284.png)

#### set 属性名称=属性值

我们可以通过设置 set key=value 的这种模式，来定义属性值，例如` set userName=kingcall;` key 如果已经存在则覆盖，不存在则创建，key 可以是任意的字符串

设置成功之后，我们可以通过`set`来查看属性的设置情况

![image-20201223220121426](https://kingcall.oss-cn-hangzhou.aliyuncs.com/blog/img/2020/12/23/22:01:21-image-20201223220121426.png)

下面我们看一个很有用的设置`set hive.cli.print.header=true` 输出表头

![image-20201223220453158](https://kingcall.oss-cn-hangzhou.aliyuncs.com/blog/img/2020/12/23/22:04:53-image-20201223220453158.png)

这个我们更多的是用在资源的调节上，例如`set mapred.reduce.tasks=32;`

#### reset

 重置参数，这个就没有什演示的必要了

#### set -v

只列出Hadoop 和 Hive 的属性值，因为我们知道我们可以定义任意属性，所以这个命令还是有意义的。



### 资源管理



#### add

添加文件、jar 包、分布式文件到分布式缓存中，如果你写过UDF 的话，你就应该知道这个命令了

```
add FILE[S] <ivyurl> <ivyurl>* 
add JAR[S] <ivyurl> <ivyurl>* 
add ARCHIVE[S]<ivyurl> <ivyurl>*
```

创建新表，准备数据

```sql
CREATE TABLE ods.u_data (
  userid INT,
  movieid INT,
  rating INT,
  unixtime STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
STORED AS TEXTFILE;
```

```shell
wget http://files.grouplens.org/datasets/movielens/ml-100k.zip
unzip ml-100k.zip
```

```
LOAD DATA LOCAL INPATH '/Users/liuwenqiang/ml-100k/u.data' OVERWRITE INTO TABLE ods.u_data;
```

创建脚本

```python
import sys
import datetime

for line in sys.stdin:
  line = line.strip()
  userid, movieid, rating, unixtime = line.split('\t')
  weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday()
  print('\t'.join([userid, movieid, rating, str(weekday)]))
```

创建新表、加载脚本、执行sql 使用脚本

```sql
CREATE TABLE u_data_new (
  userid INT,
  movieid INT,
  rating INT,
  weekday INT)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t';

add file /Users/liuwenqiang/weekday_mapper.py;

INSERT OVERWRITE TABLE u_data_new
SELECT
  TRANSFORM (userid, movieid, rating, unixtime)
  USING 'python weekday_mapper.py'
  AS (userid, movieid, rating, weekday)
FROM u_data;
```



#### list

列出被增加到分布式缓存中的资源

```
list FILE[S]
list JAR[S]
list ARCHIVE[S]
```

![image-20201223222012134](https://kingcall.oss-cn-hangzhou.aliyuncs.com/blog/img/2020/12/23/22:20:12-image-20201223222012134.png)

#### delete

删除被增加到分布式缓存中的资源

![image-20201223222129059](https://kingcall.oss-cn-hangzhou.aliyuncs.com/blog/img/2020/12/23/22:21:29-image-20201223222129059.png)

### quit/exit

退出终端



### ！执行模式

这个还是比较有用的，可以让我们在hive 的命令行里面执行shell 的命令。

#### 执行shell 命令

例如下面我执行这个test.sql 的时候，我用的不是全路径，那我是怎么知道它是在当前路径(你执行hive命令的目录)下的呢

![image-20201223212507813](https://kingcall.oss-cn-hangzhou.aliyuncs.com/blog/img/2020/12/23/21:25:08-image-20201223212507813.png)

 那是因为我执行了ls 看了一下，确定是它是在当前目录的

![image-20201223212843764](https://kingcall.oss-cn-hangzhou.aliyuncs.com/blog/img/2020/12/23/21:28:44-image-20201223212843764.png)

#### dfs

指向Hadoop 相关的命令，你可以明显的感觉到比你在shell 里面执，下面我们用两种方式执行了一个同样目的的命令，第一种是我们介绍的执行shell 命令的模式，第二种是我们现在介绍的这种

![image-20201223213332438](https://kingcall.oss-cn-hangzhou.aliyuncs.com/blog/img/2020/12/23/21:33:33-image-20201223213332438.png)

### compile



## 总结

| 命令                                                         | 注释                                                         |
| :----------------------------------------------------------- | :----------------------------------------------------------- |
| source FILE <filepath>                                       | 在CLI中执行一个脚本。                                        |
| set <key>=<value>                                            | 为一个特定的配置变量设置一个值。注：如果你忘了怎么拼写那个变量的名字，CLI将不会展示一个错误。 |
| set -v                                                       | 打印所有的Hadoop和Hive的配置变量。                           |
| set                                                          | Prints a list of configuration variables that are overridden by the user or Hive.打一个被用户或者Hive重写的配置变量列表。 |
| reset                                                        | 重新设置配置项为默认值（Hive 0.10请查阅HIVE-3202）。任何在命令行被使用设置命令或者-hiveonf的参数将会恢复默认值。注：由于历史原因，这个不适用于那些在命令行中用hiveconf.做前缀的关键字配置参数。 |
| quit  exit                                                   | 使用quit或者exit命令来离开交互shell窗口                      |
| list FILE[S] <filepath>*  list JAR[S] <filepath>*  list ARCHIVE[S] <filepath>* | 检查是否给定的资源都已经增加到分布式是缓存中了。查阅[Hive Resource](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Cli#LanguageManualCli-HiveResources)可以看到更多的信息。 |
| list FILE[S]  list JAR[S]  list ARCHIVE[S]                   | 列出被增加到分布式缓存中的资源。查阅[Hive Resources](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Cli#LanguageManualCli-HiveResources)可以得到更多信息 |
| dfs <dfs command>                                            | 在Hive shell窗口执行一个dfs命令。                            |
| delete FILE[S] <ivyurl> <ivyurl>*  delete JAR[S] <ivyurl> <ivyurl>*  delete ARCHIVE[S] <ivyurl> <ivyurl>* | [Hive1.2.0](https://issues.apache.org/jira/browse/HIVE-9664)shanchu分布式缓存中的资源。查阅[HiveResource](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Cli#LanguageManualCli-HiveResources)获取更多信息 |
| delete FILE[S] <filepath>*  delete JAR[S] <filepath>*  delete ARCHIVE[S] <filepath>* | 删除分布式缓存中的资源                                       |
| compile `<groovy string>` AS GROOVY NAMED <name>             | 这里允许内联代码被编译像UDF（Hive [0.13.0](https://issues.apache.org/jira/browse/HIVE-5252)）一样被使用。查阅Hive中的[动态编译器](https://cwiki.apache.org/confluence/download/attachments/27362054/HiveContrib-Nov13-groovy_plus_hive.pptx?version=1&modificationDate=1385171856000&api=v2) |
| add FILE[S] <ivyurl> <ivyurl>*  add JAR[S] <ivyurl> <ivyurl>*  add ARCHIVE[S]<ivyurl> <ivyurl>* | [Hive 1.2.0](https://issues.apache.org/jira/browse/HIVE-9664)中使用一个类似格式：ivy://group:module:version?query_string的lvy URL增加一个或者更多个文件、jar包或者压缩包到分布式缓存的资源列表中。查阅[Hive Resources](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Cli#LanguageManualCli-HiveResources)获取更多信息。 |
| add FILE[S] <filepath> <filepath>*  add JAR[S] <filepath> <filepath>*  add ARCHIVE[S] <filepath> <filepath>* | Adds one or more files, jars, or archives to the list of resources in the distributed cache. See [Hive Resources](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Cli#LanguageManualCli-HiveResources) for more information.增加一个或者多个文件，jar包，或者压缩包到分布式缓存中。chax |
| <query string>                                               | 执行Hive查询并且打印标准输出。                               |
| ! <command>                                                  | 在Hive中执行一个Shell命令                                    |

